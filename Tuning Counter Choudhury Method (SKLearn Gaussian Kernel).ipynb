{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7ffc2e",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70799958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "import math\n",
    "import datetime, os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from functions.common_function import *\n",
    "from functions.build_tuner_model import build_tuner_model\n",
    "from functions.dataset_interpolation import dataset_interpolation_sklearn\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "'''Enabling plotting of graphs just below the plotting commands'''\n",
    "%matplotlib inline\n",
    "'''Enabling the disply of all rows and columns within the dataframe'''\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403514e",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce73a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature = 8\n",
    "cat_col = [4, 5]\n",
    "num_ori_feature = num_feature - len(cat_col)\n",
    "num_target = 3\n",
    "bandwidth = 100\n",
    "num_epochs = 10000\n",
    "num_folds = 4\n",
    "directory_name = \"Tuning_Counter_Choudhury_Method_SKLearn_Gaussian_Kernel\"\n",
    "\n",
    "limit = pd.DataFrame({'lower' : [303, 20, 0, 2, 6, 1.5, 122, 1236, 14], \\\n",
    "                     'higher' : [840, 44, 17, 5, 8, 2, 408, 3240, 101], \\\n",
    "                     'ref' : [530, 40, 14, 3.2, 6, 1.8, np.nan, np.nan, np.nan]})\n",
    "\n",
    "'''Importing Dataset'''\n",
    "dataset = pd.read_csv(\"Dataset/Choudhury_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fac32",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76e68d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 05m 03s]\n",
      "val_loss: 0.00698390556499362\n",
      "\n",
      "Best val_loss So Far: 0.005455001723021269\n",
      "Total elapsed time: 01h 13m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in keras_tuner\\Tuning_Counter_Choudhury_Method_SKLearn_Gaussian_Kernel\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 15\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 8\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 9\n",
      "num_of_neurons17: 2\n",
      "num_of_neurons18: 15\n",
      "num_of_neurons19: 2\n",
      "Score: 0.005455001723021269\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 15\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 2\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 2\n",
      "num_of_neurons17: 2\n",
      "num_of_neurons18: 2\n",
      "num_of_neurons19: 2\n",
      "Score: 0.005678983870893717\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 9\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 5\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 14\n",
      "num_of_neurons17: 7\n",
      "num_of_neurons18: 3\n",
      "num_of_neurons19: 3\n",
      "Score: 0.0065312692895531654\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 13\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 14\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 4\n",
      "num_of_neurons17: 9\n",
      "num_of_neurons18: 5\n",
      "num_of_neurons19: 14\n",
      "Score: 0.006841862108558416\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 15\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 15\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 5\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 15\n",
      "num_of_neurons17: 8\n",
      "num_of_neurons18: 12\n",
      "num_of_neurons19: 15\n",
      "Score: 0.00698390556499362\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 11\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 2\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 4\n",
      "num_of_neurons17: 12\n",
      "num_of_neurons18: 14\n",
      "num_of_neurons19: 6\n",
      "Score: 0.008362098596990108\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 6\n",
      "num_of_layers: 10\n",
      "num_of_neurons0: 12\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "Score: 0.009330855682492256\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 1\n",
      "num_of_layers: 1\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 5\n",
      "num_of_neurons11: 4\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 5\n",
      "num_of_neurons14: 2\n",
      "num_of_neurons15: 12\n",
      "Score: 0.4725954830646515\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 14\n",
      "num_of_layers: 16\n",
      "num_of_neurons0: 13\n",
      "num_of_neurons1: 4\n",
      "num_of_neurons2: 13\n",
      "num_of_neurons3: 7\n",
      "num_of_neurons4: 12\n",
      "num_of_neurons5: 5\n",
      "num_of_neurons6: 9\n",
      "num_of_neurons7: 11\n",
      "num_of_neurons8: 14\n",
      "num_of_neurons9: 9\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 2\n",
      "num_of_neurons15: 2\n",
      "Score: 0.5637387633323669\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 4\n",
      "num_of_layers: 20\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 13\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 2\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 2\n",
      "num_of_neurons17: 2\n",
      "num_of_neurons18: 2\n",
      "num_of_neurons19: 2\n",
      "Score: 0.5637387633323669\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:tensorflow:Assets written to: Model\\Tuning_Counter_Choudhury_Method_SKLearn_Gaussian_Kernel\\model_4\\assets\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "'''Dataset splitting before interpolation to not introduce data leakage '''\n",
    "x_train, x_test, y_train, y_test = tt_split(dataset, 0.2, num_feature, num_target)\n",
    "\n",
    "'''Merging of features and targets datasets'''\n",
    "train_dataset = merge_x_y(x_train, y_train)\n",
    "test_dataset = merge_x_y(x_test, y_test)\n",
    "\n",
    "'''Dataset Interpolation'''\n",
    "interpolated_dataset = dataset_interpolation_sklearn(train_dataset, num_ori_feature, num_target, limit)\n",
    "\n",
    "'''Converting Categorical Data into binary representation'''\n",
    "converted_dataset = convert_cat(interpolated_dataset, cat_col, num_ori_feature, num_target, [interpolated_dataset.iloc[:, 4].unique(), interpolated_dataset.iloc[:, 5].unique()])\n",
    "converted_test_dataset = convert_cat(test_dataset, cat_col, num_ori_feature, num_target, [dataset.iloc[:, 4].unique(), dataset.iloc[:, 5].unique()])\n",
    "\n",
    "'''Normalising dataset using Min Max present in the train set'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(converted_dataset)\n",
    "\n",
    "'''Normalisation of training and testing set'''\n",
    "normalised_train_dataset = pd.DataFrame(scaler.transform(converted_dataset), columns = get_col_names(converted_dataset))\n",
    "normalised_test_dataset = pd.DataFrame(scaler.transform(converted_test_dataset), columns = get_col_names(converted_dataset))\n",
    "\n",
    "x_train = normalised_train_dataset.iloc[:, 0: num_feature]\n",
    "y_train =  normalised_train_dataset.iloc[:, num_feature: num_feature + num_target]\n",
    "x_test = normalised_test_dataset.iloc[:, 0: num_feature]\n",
    "y_test = normalised_test_dataset.iloc[:, num_feature: num_feature + num_target]\n",
    "\n",
    "\n",
    "'''Cross Validation'''\n",
    "kfold =KFold(n_splits = num_folds, shuffle = True)\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    tuner = kt.BayesianOptimization(build_tuner_model, objective = 'val_loss', max_trials = 10, directory = 'keras_tuner', project_name = directory_name, overwrite = True)\n",
    "    tuner.search_space_summary()\n",
    "    tuner.search(x_train.iloc[train], y_train.iloc[train], epochs = num_epochs, validation_data= (x_train.iloc[test], y_train.iloc[test]))\n",
    "    tuner.results_summary()\n",
    "    best_hps = tuner.get_best_models()[0]\n",
    "    best_hps.save(f\"Model\\{directory_name}\\model_{fold_no}\")\n",
    "    print(\"Saved model to disk\")\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441830b",
   "metadata": {},
   "source": [
    "#  Loading of Models and Evaluate Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e4f760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6493 - mean_squared_error: 1.2692 - root_mean_squared_error: 1.1266\n",
      "Mean Absolute Error for model 1(Loss):  0.6493167281150818\n",
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6473 - mean_squared_error: 1.2709 - root_mean_squared_error: 1.1274\n",
      "Mean Absolute Error for model 2(Loss):  0.6472587585449219\n",
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6646 - mean_squared_error: 1.2721 - root_mean_squared_error: 1.1279\n",
      "Mean Absolute Error for model 3(Loss):  0.6646442413330078\n",
      "Loaded model from disk\n",
      "WARNING:tensorflow:5 out of the last 50004 calls to <function Model.make_test_function.<locals>.test_function at 0x0000020300DE98B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6391 - mean_squared_error: 1.2441 - root_mean_squared_error: 1.1154\n",
      "Mean Absolute Error for model 4(Loss):  0.6390843987464905\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 1 - MAE: 0.6493167281150818\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 2 - MAE: 0.6472587585449219\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 3 - MAE: 0.6646442413330078\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 4 - MAE: 0.6390843987464905\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 0.6500760316848755 - Standard Deviation: 0.009240702041245724\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(1, 5):\n",
    "    '''Loads the Best Model Trained using Cross Validation'''\n",
    "    loaded_model = keras.models.load_model(f\"Model\\{directory_name}\\model_{i}\")\n",
    "\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    '''Compilation of the model with its corresponding weights, followed by the evaluation of the model using test set'''\n",
    "    loaded_model.compile(loss = 'MeanAbsoluteError',\\\n",
    "                        optimizer = 'SGD',\\\n",
    "                        metrics = [tf.keras.metrics.MeanSquaredError(),\\\n",
    "                        tf.keras.metrics.RootMeanSquaredError()])\n",
    "    result = loaded_model.evaluate(x_test, y_test, batch_size = 128)\n",
    "    print(f\"Mean Absolute Error for model {i}(Loss): \", result[0])\n",
    "    results.append(result[0])\n",
    "\n",
    "'''Provide average score'''\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(results)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Iteration {i+1} - MAE: {results[i]}') \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> MAE: {np.mean(results)} - Standard Deviation: {np.std(results)}')\n",
    "print('------------------------------------------------------------------------')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236967ca",
   "metadata": {},
   "source": [
    "# Visualisation of Predictions using Best Model from Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566e738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9c69c_\">\n",
       "  <caption>Element 1</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9c69c_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_9c69c_row0_col0\" class=\"data row0 col0\" >256.354529</td>\n",
       "      <td id=\"T_9c69c_row0_col1\" class=\"data row0 col1\" >209.554787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c69c_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_9c69c_row1_col0\" class=\"data row1 col0\" >1953.649646</td>\n",
       "      <td id=\"T_9c69c_row1_col1\" class=\"data row1 col1\" >1865.561867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9c69c_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_9c69c_row2_col0\" class=\"data row2 col0\" >39.379782</td>\n",
       "      <td id=\"T_9c69c_row2_col1\" class=\"data row2 col1\" >35.865465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20300ebe288>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_76df6_\">\n",
       "  <caption>Element 2</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_76df6_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_76df6_row0_col0\" class=\"data row0 col0\" >94.032827</td>\n",
       "      <td id=\"T_76df6_row0_col1\" class=\"data row0 col1\" >230.028237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_76df6_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_76df6_row1_col0\" class=\"data row1 col0\" >2328.182780</td>\n",
       "      <td id=\"T_76df6_row1_col1\" class=\"data row1 col1\" >2025.354198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_76df6_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_76df6_row2_col0\" class=\"data row2 col0\" >46.892537</td>\n",
       "      <td id=\"T_76df6_row2_col1\" class=\"data row2 col1\" >34.977672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20339d4d108>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_45ef4_\">\n",
       "  <caption>Element 3</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_45ef4_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_45ef4_row0_col0\" class=\"data row0 col0\" >233.399945</td>\n",
       "      <td id=\"T_45ef4_row0_col1\" class=\"data row0 col1\" >219.774484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45ef4_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_45ef4_row1_col0\" class=\"data row1 col0\" >1993.922026</td>\n",
       "      <td id=\"T_45ef4_row1_col1\" class=\"data row1 col1\" >2031.319387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45ef4_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_45ef4_row2_col0\" class=\"data row2 col0\" >44.388285</td>\n",
       "      <td id=\"T_45ef4_row2_col1\" class=\"data row2 col1\" >42.826582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2030092af08>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1b43f_\">\n",
       "  <caption>Element 4</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1b43f_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_1b43f_row0_col0\" class=\"data row0 col0\" >136.662769</td>\n",
       "      <td id=\"T_1b43f_row0_col1\" class=\"data row0 col1\" >217.648729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b43f_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_1b43f_row1_col0\" class=\"data row1 col0\" >-817.090095</td>\n",
       "      <td id=\"T_1b43f_row1_col1\" class=\"data row1 col1\" >2008.718304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b43f_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_1b43f_row2_col0\" class=\"data row2 col0\" >23.102147</td>\n",
       "      <td id=\"T_1b43f_row2_col1\" class=\"data row2 col1\" >30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2030092af48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 1 - Pearson Correlation: 0.9998452742432522\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 2 - Pearson Correlation: 0.9974859257193649\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 3 - Pearson Correlation: 0.9999716169527251\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 4 - Pearson Correlation: -0.9810069605290745\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Average Pearsons Correlation: 0.5040739640965668 - Standard Deviation: 0.8574124431439363\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_model_index = results.index(min(results)) + 1\n",
    "\n",
    "'''Loads the Best Model Trained using Cross Validation'''\n",
    "loaded_model = keras.models.load_model(f\"Model\\{directory_name}\\model_{best_model_index}\")\n",
    "\n",
    "prediction = pd.DataFrame(loaded_model.predict(x_test), columns = get_col_names(y_test))\n",
    "\n",
    "'''Preparation to Rescale target values'''\n",
    "min_y = dataset.iloc[:, num_ori_feature: num_ori_feature + num_target].min().to_list()\n",
    "max_y = dataset.iloc[:, num_ori_feature: num_ori_feature + num_target].max().to_list()\n",
    "\n",
    "corr_list = []\n",
    "'''Tabulating the differences of Expected and Predictions made by the ANN Model'''\n",
    "for i in range(len(x_test)):\n",
    "    '''Rescaling of normalised data'''\n",
    "    expected = pd.DataFrame(inverse_transform(y_test.iloc[i].to_list(), max_y, min_y))\n",
    "    predicted = pd.DataFrame(inverse_transform(prediction.iloc[i].to_list(), max_y, min_y))\n",
    "    comparison_df = pd.concat([expected, predicted], axis = 1)\n",
    "    comparison_df.columns = ['Expected', 'Predicted']\n",
    "    comparison_df.index = get_col_names(y_test)\n",
    "    display(comparison_df.style.set_caption(f\"Element {i + 1}\"))\n",
    "    corr, _ = pearsonr(expected.iloc[:, 0].tolist(), predicted.iloc[:, 0].tolist())\n",
    "    corr_list.append(corr)\n",
    "    \n",
    "'''Provide average score'''\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(corr_list)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Iteration {i+1} - Pearson Correlation: {corr_list[i]}') \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Average Pearsons Correlation: {np.mean(corr_list)} - Standard Deviation: {np.std(corr_list)}')\n",
    "print('------------------------------------------------------------------------')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
