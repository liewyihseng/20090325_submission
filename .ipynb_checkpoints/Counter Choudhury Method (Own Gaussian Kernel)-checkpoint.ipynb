{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "import datetime, os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "from kernels.gaussian_kernel_regression import gaussian_kernel_regression\n",
    "from functions.common_function import *\n",
    "from functions.initialize_model import initialize_model_expanded\n",
    "from functions.cross_validation import cross_validation_split\n",
    "from functions.dataset_interpolation import dataset_interpolation_own\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "'''Enabling plotting of graphs just below the plotting commands'''\n",
    "%matplotlib inline\n",
    "'''Enabling the disply of all rows and columns within the dataframe'''\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature = 8\n",
    "cat_col = [4, 5]\n",
    "num_ori_feature = num_feature - len(cat_col)\n",
    "num_target = 3\n",
    "bandwidth = 100\n",
    "num_epochs = 10000\n",
    "num_batch = 10\n",
    "num_folds = 4\n",
    "directory_name = \"Counter_Choudhury_Method_Own_Gaussian_Kernel\"\n",
    "\n",
    "limit = pd.DataFrame({'lower' : [303, 20, 0, 2, 6, 1.5, 122, 1236, 14], \\\n",
    "                     'higher' : [840, 44, 17, 5, 8, 2, 408, 3240, 101], \\\n",
    "                     'ref' : [530, 40, 14, 3.2, 6, 1.8, np.nan, np.nan, np.nan]})\n",
    "\n",
    "'''Import dataset'''\n",
    "dataset = pd.read_csv(\"Dataset/Choudhury_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Iterate 10 times to mimic Cross Validation'''\n",
    "MAE = []\n",
    "MSE = []\n",
    "RMSE = []\n",
    "\n",
    "folds = cross_validation_split(dataset, num_folds)\n",
    "\n",
    "\n",
    "'''Cross Validation'''\n",
    "for i in range(len(folds)):\n",
    "    test_index = [folds[i]]\n",
    "    train_index = [fold for fold in folds if fold not in test_index]\n",
    "    test_index = list(itertools.chain.from_iterable(test_index))\n",
    "    train_index = list(itertools.chain.from_iterable(train_index))\n",
    "    \n",
    "    '''Dataset Interpolation'''\n",
    "    interpolated_dataset = dataset_interpolation_own(dataset.iloc[train_index], num_ori_feature, num_target, limit, bandwidth)\n",
    "    \n",
    "    '''Converting Categorical Data into binary representation'''\n",
    "    converted_dataset = convert_cat(interpolated_dataset, cat_col, num_ori_feature, num_target, [interpolated_dataset.iloc[:, 4].unique(), interpolated_dataset.iloc[:, 5].unique()])\n",
    "    converted_test_dataset = convert_cat(dataset.iloc[test_index], cat_col, num_ori_feature, num_target, [dataset.iloc[:, 4].unique(), dataset.iloc[:, 5].unique()])\n",
    "\n",
    "    '''Normalising dataset using Min Max present in the train set'''\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(converted_dataset)\n",
    "    normalised_train_dataset = pd.DataFrame(scaler.transform(converted_dataset), columns = get_col_names(converted_dataset))\n",
    "    normalised_test_dataset = pd.DataFrame(scaler.transform(converted_test_dataset), columns = get_col_names(converted_dataset))\n",
    "    \n",
    "    x_train = normalised_train_dataset.iloc[:, 0: num_feature]\n",
    "    y_train =  normalised_train_dataset.iloc[:, num_feature: num_feature + num_target]\n",
    "    x_test = normalised_test_dataset.iloc[:, 0: num_feature]\n",
    "    y_test = normalised_test_dataset.iloc[:, num_feature: num_feature + num_target]\n",
    "\n",
    "    model = initialize_model_expanded(num_feature, num_target, 'relu')\n",
    "    \n",
    "    '''Model Fitting'''\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for iteration {i + 1} ...')\n",
    "    \n",
    "    '''Initializing early stopping that prevents overfitting\n",
    "    and tensorboard for visualizing machine learning workflow'''\n",
    "    early_stopping = EarlyStopping(monitor = 'loss', mode = 'min', verbose = 1, patience = 1800)\n",
    "    logdir = os.path.join(f\"logs/{directory_name}\", \"ANN_\" + str(i + 1))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, \\\n",
    "                                                         histogram_freq = 1,\n",
    "                                                         write_graph = True,\\\n",
    "                                                         write_images = True)\n",
    "    \n",
    "    history = model.fit(x_train,\\\n",
    "                        y_train,\n",
    "                        epochs = num_epochs,\\\n",
    "                        batch_size = num_batch,\\\n",
    "                        callbacks = [tensorboard_callback, early_stopping])\n",
    "    \n",
    "    '''Index 0 of result is represented by Mean Absolute Error'''\n",
    "    result = model.evaluate(x_test, y_test, batch_size = 128)\n",
    "    MAE.append(result[0])\n",
    "    MSE.append(result[1])\n",
    "    RMSE.append(result[2])\n",
    "    model.save(f\"Model\\{directory_name}\\model_{i + 1}\")\n",
    "    print(\"Saved model to disk\")    \n",
    "    print(f'Score for fold {i + 1}: {model.metrics_names[0]} of {result[0]}')\n",
    "\n",
    "'''Provide average score'''\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(MAE)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Iteration {i+1} - MAE: {MAE[i]} - MSE: {MSE[i]}- RMSE: {RMSE[i]}') \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> MAE: {np.mean(MAE)} - Standard Deviation: {np.std(MAE)}')\n",
    "print(f'> MSE: {np.mean(MSE)}')\n",
    "print(f'> RMSE: {np.mean(RMSE)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loads the Best Model Trained using Cross Validation'''\n",
    "loaded_model = keras.models.load_model(f\"Model\\{directory_name}\\model_{MAE.index(min(MAE)) + 1}\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "'''Compilation of the model with its corresponding weights, followed by the evaluation of the model using test set'''\n",
    "loaded_model.compile(loss = 'MeanAbsoluteError',\\\n",
    "                    optimizer = 'SGD',\\\n",
    "                    metrics = [tf.keras.metrics.MeanSquaredError(),\\\n",
    "                    tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Predictions using Best Model from Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Converting Categorical Data into binary representation'''\n",
    "converted_visualise_dataset = convert_cat(dataset, cat_col, num_ori_feature, num_target, [dataset.iloc[:, 4].unique(), dataset.iloc[:, 5].unique()])\n",
    "\n",
    "'''Feature Target Splitting'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(converted_visualise_dataset)\n",
    "normalised_dataset = pd.DataFrame(scaler.transform(converted_visualise_dataset), columns = get_col_names(converted_visualise_dataset))\n",
    "feature, target = x_y_split(normalised_dataset, num_feature, num_target)\n",
    "prediction = pd.DataFrame(loaded_model.predict(feature), columns = get_col_names(target))\n",
    "\n",
    "'''Preparation to Rescale target values'''\n",
    "min_y = dataset.iloc[:, num_ori_feature: num_ori_feature + num_target].min().to_list()\n",
    "max_y = dataset.iloc[:, num_ori_feature: num_ori_feature + num_target].max().to_list()\n",
    "\n",
    "corr_list = []\n",
    "'''Tabulating the differences of Expected and Predictions made by the ANN Model'''\n",
    "for i in range(len(feature)):\n",
    "    '''Rescaling of normalised data'''\n",
    "    expected = pd.DataFrame(inverse_transform(target.iloc[i].to_list(), max_y, min_y))\n",
    "    predicted = pd.DataFrame(inverse_transform(prediction.iloc[i].to_list(), max_y, min_y))\n",
    "    comparison_df = pd.concat([expected, predicted], axis = 1)\n",
    "    comparison_df.columns = ['Expected', 'Predicted']\n",
    "    comparison_df.index = get_col_names(target)\n",
    "    display(comparison_df.style.set_caption(f\"Element {i + 1}\"))\n",
    "    corr, _ = pearsonr(expected.iloc[:, 0].tolist(), predicted.iloc[:, 0].tolist())\n",
    "    corr_list.append(corr)\n",
    "\n",
    "'''Provide average score'''\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(corr_list)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Iteration {i+1} - Pearson Correlation: {corr_list[i]}') \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Average Pearsons Correlation: {np.mean(corr_list)} - Standard Deviation: {np.std(corr_list)}')\n",
    "print('------------------------------------------------------------------------')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
