{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aab11f1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbbbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "import math\n",
    "import datetime, os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kernels.gaussian_kernel_regression import gaussian_kernel_regression\n",
    "from functions.common_function import *\n",
    "from functions.build_tuner_model import build_tuner_model\n",
    "from functions.dataset_interpolation import dataset_interpolation_own\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "'''Enabling plotting of graphs just below the plotting commands'''\n",
    "%matplotlib inline\n",
    "'''Enabling the disply of all rows and columns within the dataframe'''\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e4ae9",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26d21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature = 8\n",
    "cat_col = [4, 5]\n",
    "num_ori_feature = num_feature - len(cat_col)\n",
    "num_target = 3\n",
    "bandwidth = 100\n",
    "num_epochs = 10000\n",
    "num_folds = 4\n",
    "directory_name = \"Tuning_Counter_Choudhury_Method_Own_Gaussian_Kernel\"\n",
    "\n",
    "limit = pd.DataFrame({'lower' : [303, 20, 0, 2, 6, 1.5, 122, 1236, 14], \\\n",
    "                     'higher' : [840, 44, 17, 5, 8, 2, 408, 3240, 101], \\\n",
    "                     'ref' : [530, 40, 14, 3.2, 6, 1.8, np.nan, np.nan, np.nan]})\n",
    "\n",
    "'''Importing Dataset'''\n",
    "dataset = pd.read_csv(\"Dataset/Choudhury_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404323e",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120690b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 05m 28s]\n",
      "val_loss: 0.8122020363807678\n",
      "\n",
      "Best val_loss So Far: 0.013890246860682964\n",
      "Total elapsed time: 01h 39m 00s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in keras_tuner\\Tuning_Counter_Choudhury_Method_Own_Gaussian_Kernel\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 10\n",
      "num_of_layers: 6\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 6\n",
      "num_of_neurons3: 12\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 7\n",
      "num_of_neurons6: 15\n",
      "num_of_neurons7: 12\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 15\n",
      "num_of_neurons11: 15\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 11\n",
      "num_of_neurons17: 15\n",
      "num_of_neurons18: 7\n",
      "Score: 0.013890246860682964\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 6\n",
      "num_of_layers: 8\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 6\n",
      "num_of_neurons3: 12\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 11\n",
      "num_of_neurons6: 15\n",
      "num_of_neurons7: 9\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 4\n",
      "num_of_neurons10: 15\n",
      "num_of_neurons11: 15\n",
      "num_of_neurons12: 4\n",
      "num_of_neurons13: 6\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 8\n",
      "num_of_neurons17: 15\n",
      "num_of_neurons18: 11\n",
      "Score: 0.014133322052657604\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 6\n",
      "num_of_layers: 12\n",
      "num_of_neurons0: 6\n",
      "num_of_neurons1: 3\n",
      "num_of_neurons2: 9\n",
      "num_of_neurons3: 7\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 6\n",
      "num_of_neurons6: 10\n",
      "num_of_neurons7: 8\n",
      "num_of_neurons8: 14\n",
      "num_of_neurons9: 5\n",
      "num_of_neurons10: 14\n",
      "num_of_neurons11: 14\n",
      "num_of_neurons12: 3\n",
      "num_of_neurons13: 4\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 5\n",
      "num_of_neurons17: 13\n",
      "num_of_neurons18: 6\n",
      "Score: 0.29828572273254395\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 10\n",
      "num_of_layers: 6\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 8\n",
      "num_of_neurons3: 15\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 11\n",
      "num_of_neurons6: 15\n",
      "num_of_neurons7: 12\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 8\n",
      "num_of_neurons10: 15\n",
      "num_of_neurons11: 15\n",
      "num_of_neurons12: 8\n",
      "num_of_neurons13: 13\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 15\n",
      "num_of_neurons17: 15\n",
      "num_of_neurons18: 11\n",
      "Score: 0.5805097222328186\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 3\n",
      "num_of_layers: 2\n",
      "num_of_neurons0: 15\n",
      "num_of_neurons1: 2\n",
      "Score: 0.8122020363807678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 8\n",
      "num_of_layers: 11\n",
      "num_of_neurons0: 13\n",
      "num_of_neurons1: 11\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 2\n",
      "num_of_neurons4: 2\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 2\n",
      "num_of_neurons7: 2\n",
      "num_of_neurons8: 2\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 2\n",
      "Score: 0.8122020363807678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 6\n",
      "num_of_layers: 19\n",
      "num_of_neurons0: 10\n",
      "num_of_neurons1: 10\n",
      "num_of_neurons2: 15\n",
      "num_of_neurons3: 3\n",
      "num_of_neurons4: 6\n",
      "num_of_neurons5: 2\n",
      "num_of_neurons6: 6\n",
      "num_of_neurons7: 9\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 7\n",
      "num_of_neurons10: 14\n",
      "num_of_neurons11: 2\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 2\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 2\n",
      "num_of_neurons17: 2\n",
      "num_of_neurons18: 2\n",
      "Score: 0.8122020363807678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 1\n",
      "num_of_layers: 8\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 12\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 15\n",
      "num_of_neurons6: 15\n",
      "num_of_neurons7: 6\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 15\n",
      "num_of_neurons11: 15\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 2\n",
      "num_of_neurons17: 15\n",
      "num_of_neurons18: 15\n",
      "Score: 0.8122020363807678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 8\n",
      "num_of_layers: 10\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 14\n",
      "num_of_neurons3: 12\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 9\n",
      "num_of_neurons6: 15\n",
      "num_of_neurons7: 4\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 15\n",
      "num_of_neurons11: 15\n",
      "num_of_neurons12: 2\n",
      "num_of_neurons13: 2\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 13\n",
      "num_of_neurons17: 15\n",
      "num_of_neurons18: 9\n",
      "Score: 0.8122020363807678\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units1: 9\n",
      "num_of_layers: 6\n",
      "num_of_neurons0: 2\n",
      "num_of_neurons1: 2\n",
      "num_of_neurons2: 2\n",
      "num_of_neurons3: 10\n",
      "num_of_neurons4: 15\n",
      "num_of_neurons5: 6\n",
      "num_of_neurons6: 15\n",
      "num_of_neurons7: 15\n",
      "num_of_neurons8: 15\n",
      "num_of_neurons9: 2\n",
      "num_of_neurons10: 15\n",
      "num_of_neurons11: 15\n",
      "num_of_neurons12: 4\n",
      "num_of_neurons13: 9\n",
      "num_of_neurons14: 15\n",
      "num_of_neurons15: 2\n",
      "num_of_neurons16: 5\n",
      "num_of_neurons17: 15\n",
      "num_of_neurons18: 6\n",
      "Score: 0.8122020363807678\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:tensorflow:Assets written to: Model\\Tuning_Counter_Choudhury_Method_Own_Gaussian_Kernel\\model_4\\assets\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "'''Dataset splitting before interpolation to not introduce data leakage '''\n",
    "x_train, x_test, y_train, y_test = tt_split(dataset, 0.2, num_feature, num_target)\n",
    "\n",
    "'''Merging of features and targets datasets'''\n",
    "train_dataset = merge_x_y(x_train, y_train)\n",
    "test_dataset = merge_x_y(x_test, y_test)\n",
    "\n",
    "'''Dataset Interpolation'''\n",
    "interpolated_dataset = dataset_interpolation_own(train_dataset, num_ori_feature, num_target, limit, bandwidth)\n",
    "\n",
    "'''Converting Categorical Data into binary representation'''\n",
    "converted_dataset = convert_cat(interpolated_dataset, cat_col, num_ori_feature, num_target, [interpolated_dataset.iloc[:, 4].unique(), interpolated_dataset.iloc[:, 5].unique()])\n",
    "converted_test_dataset = convert_cat(test_dataset, cat_col, num_ori_feature, num_target, [dataset.iloc[:, 4].unique(), dataset.iloc[:, 5].unique()])\n",
    "\n",
    "'''Normalising dataset using Min Max present in the train set'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(converted_dataset)\n",
    "\n",
    "'''Normalisation of training and testing set'''\n",
    "normalised_train_dataset = pd.DataFrame(scaler.transform(converted_dataset), columns = get_col_names(converted_dataset))\n",
    "normalised_test_dataset = pd.DataFrame(scaler.transform(converted_test_dataset), columns = get_col_names(converted_dataset))\n",
    "\n",
    "x_train = normalised_train_dataset.iloc[:, 0: num_feature]\n",
    "y_train =  normalised_train_dataset.iloc[:, num_feature: num_feature + num_target]\n",
    "x_test = normalised_test_dataset.iloc[:, 0: num_feature]\n",
    "y_test = normalised_test_dataset.iloc[:, num_feature: num_feature + num_target]\n",
    "\n",
    "kfold =KFold(n_splits = num_folds, shuffle = True)\n",
    "fold_no = 1\n",
    "\n",
    "'''Cross Validation'''\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    tuner = kt.BayesianOptimization(build_tuner_model, objective = 'val_loss', max_trials = 10, directory = 'keras_tuner', project_name = directory_name, overwrite = True)\n",
    "    tuner.search_space_summary()\n",
    "    tuner.search(x_train.iloc[train], y_train.iloc[train], epochs = num_epochs, validation_data= (x_train.iloc[test], y_train.iloc[test]))\n",
    "    tuner.results_summary()\n",
    "    best_hps = tuner.get_best_models()[0]\n",
    "    best_hps.save(f\"Model\\{directory_name}\\model_{fold_no}\")\n",
    "    print(\"Saved model to disk\")\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e038a",
   "metadata": {},
   "source": [
    "#  Loading of Models and Evaluate Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9104df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 29.2391 - mean_squared_error: 2320.0767 - root_mean_squared_error: 48.1672\n",
      "Mean Absolute Error for model 1(Loss):  29.2391357421875\n",
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 29.2489 - mean_squared_error: 2320.4824 - root_mean_squared_error: 48.1714\n",
      "Mean Absolute Error for model 2(Loss):  29.248912811279297\n",
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 995us/step - loss: 29.2680 - mean_squared_error: 2321.4128 - root_mean_squared_error: 48.1810\n",
      "Mean Absolute Error for model 3(Loss):  29.268009185791016\n",
      "Loaded model from disk\n",
      "WARNING:tensorflow:5 out of the last 50004 calls to <function Model.make_test_function.<locals>.test_function at 0x0000015680B79558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 984us/step - loss: 29.2193 - mean_squared_error: 2318.7361 - root_mean_squared_error: 48.1533\n",
      "Mean Absolute Error for model 4(Loss):  29.21927261352539\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 1 - MAE: 29.2391357421875\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 2 - MAE: 29.248912811279297\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 3 - MAE: 29.268009185791016\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 4 - MAE: 29.21927261352539\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 29.2438325881958 - Standard Deviation: 0.01757533254376652\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(1, 5):\n",
    "    '''Loads the Best Model Trained using Cross Validation'''\n",
    "    loaded_model = keras.models.load_model(f\"Model\\{directory_name}\\model_{i}\")\n",
    "\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    '''Compilation of the model with its corresponding weights, followed by the evaluation of the model using test set'''\n",
    "    loaded_model.compile(loss = 'MeanAbsoluteError',\\\n",
    "                        optimizer = 'SGD',\\\n",
    "                        metrics = [tf.keras.metrics.MeanSquaredError(),\\\n",
    "                        tf.keras.metrics.RootMeanSquaredError()])\n",
    "    result = loaded_model.evaluate(x_test, y_test, batch_size = 128)\n",
    "    print(f\"Mean Absolute Error for model {i}(Loss): \", result[0])\n",
    "    results.append(result[0])\n",
    "\n",
    "        \n",
    "'''Provide average score'''\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(results)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Iteration {i+1} - MAE: {results[i]}') \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> MAE: {np.mean(results)} - Standard Deviation: {np.std(results)}')\n",
    "print('------------------------------------------------------------------------')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0885f",
   "metadata": {},
   "source": [
    "# Visualisation of Predictions using Best Model from Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0163ed17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ba4f1_\">\n",
       "  <caption>Element 1</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4f1_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_ba4f1_row0_col0\" class=\"data row0 col0\" >10317.725079</td>\n",
       "      <td id=\"T_ba4f1_row0_col1\" class=\"data row0 col1\" >258.322443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4f1_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_ba4f1_row1_col0\" class=\"data row1 col0\" >5392.923376</td>\n",
       "      <td id=\"T_ba4f1_row1_col1\" class=\"data row1 col1\" >2497.458731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4f1_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_ba4f1_row2_col0\" class=\"data row2 col0\" >106.419599</td>\n",
       "      <td id=\"T_ba4f1_row2_col1\" class=\"data row2 col1\" >53.996138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x156e3bfc708>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_22b92_\">\n",
       "  <caption>Element 2</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_22b92_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_22b92_row0_col0\" class=\"data row0 col0\" >7982.035833</td>\n",
       "      <td id=\"T_22b92_row0_col1\" class=\"data row0 col1\" >256.388699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22b92_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_22b92_row1_col0\" class=\"data row1 col0\" >4604.327227</td>\n",
       "      <td id=\"T_22b92_row1_col1\" class=\"data row1 col1\" >2471.604296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22b92_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_22b92_row2_col0\" class=\"data row2 col0\" >122.893340</td>\n",
       "      <td id=\"T_22b92_row2_col1\" class=\"data row2 col1\" >53.162730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x156e3bfc888>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0a069_\">\n",
       "  <caption>Element 3</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0a069_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_0a069_row0_col0\" class=\"data row0 col0\" >10025.763924</td>\n",
       "      <td id=\"T_0a069_row0_col1\" class=\"data row0 col1\" >257.534006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a069_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_0a069_row1_col0\" class=\"data row1 col0\" >5719.239024</td>\n",
       "      <td id=\"T_0a069_row1_col1\" class=\"data row1 col1\" >2491.507670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a069_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_0a069_row2_col0\" class=\"data row2 col0\" >172.314564</td>\n",
       "      <td id=\"T_0a069_row2_col1\" class=\"data row2 col1\" >53.771430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x156e3dd4ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5e6f4_\">\n",
       "  <caption>Element 4</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Expected</th>\n",
       "      <th class=\"col_heading level0 col1\" >Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5e6f4_level0_row0\" class=\"row_heading level0 row0\" >y_1</th>\n",
       "      <td id=\"T_5e6f4_row0_col0\" class=\"data row0 col0\" >5938.307743</td>\n",
       "      <td id=\"T_5e6f4_row0_col1\" class=\"data row0 col1\" >252.434650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e6f4_level0_row1\" class=\"row_heading level0 row1\" >y_2</th>\n",
       "      <td id=\"T_5e6f4_row1_col0\" class=\"data row1 col0\" >4441.169403</td>\n",
       "      <td id=\"T_5e6f4_row1_col1\" class=\"data row1 col1\" >2436.563745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e6f4_level0_row2\" class=\"row_heading level0 row2\" >y_3</th>\n",
       "      <td id=\"T_5e6f4_row2_col0\" class=\"data row2 col0\" >106.419599</td>\n",
       "      <td id=\"T_5e6f4_row2_col1\" class=\"data row2 col1\" >52.156832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x156822e0488>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 1 - Pearson Correlation: 0.0957360757915223\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 2 - Pearson Correlation: 0.15608413624964773\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 3 - Pearson Correlation: 0.14741522420819403\n",
      "------------------------------------------------------------------------\n",
      "> Iteration 4 - Pearson Correlation: 0.3425661841590225\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Average Pearsons Correlation: 0.18545040510209665 - Standard Deviation: 0.09359898723311211\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_model_index = results.index(min(results)) + 1\n",
    "\n",
    "'''Loads the Best Model Trained using Cross Validation'''\n",
    "loaded_model = keras.models.load_model(f\"Model\\{directory_name}\\model_{best_model_index}\")\n",
    "\n",
    "prediction = pd.DataFrame(loaded_model.predict(x_test), columns = get_col_names(y_test))\n",
    "\n",
    "'''Preparation to Rescale target values'''\n",
    "min_y = dataset.iloc[:, num_ori_feature: num_ori_feature + num_target].min().to_list()\n",
    "max_y = dataset.iloc[:, num_ori_feature: num_ori_feature + num_target].max().to_list()\n",
    "\n",
    "corr_list = []\n",
    "'''Tabulating the differences of Expected and Predictions made by the ANN Model'''\n",
    "for i in range(len(x_test)):\n",
    "    '''Rescaling of normalised data'''\n",
    "    expected = pd.DataFrame(inverse_transform(y_test.iloc[i].to_list(), max_y, min_y))\n",
    "    predicted = pd.DataFrame(inverse_transform(prediction.iloc[i].to_list(), max_y, min_y))\n",
    "    comparison_df = pd.concat([expected, predicted], axis = 1)\n",
    "    comparison_df.columns = ['Expected', 'Predicted']\n",
    "    comparison_df.index = get_col_names(y_test)\n",
    "    display(comparison_df.style.set_caption(f\"Element {i + 1}\"))\n",
    "    corr, _ = pearsonr(expected.iloc[:, 0].tolist(), predicted.iloc[:, 0].tolist())\n",
    "    corr_list.append(corr)\n",
    "    \n",
    "'''Provide average score'''\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(corr_list)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Iteration {i+1} - Pearson Correlation: {corr_list[i]}') \n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Average Pearsons Correlation: {np.mean(corr_list)} - Standard Deviation: {np.std(corr_list)}')\n",
    "print('------------------------------------------------------------------------')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
